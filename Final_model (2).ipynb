{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "84152514235842b5b7d79caece4c1ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4bdfe0625b64d77b7d906d8087080d8",
              "IPY_MODEL_613fede7d982426ab5b93dc985f444d3",
              "IPY_MODEL_9bcb8d7cc9ec4d9fb5f8fc6414cf27c6"
            ],
            "layout": "IPY_MODEL_92f196fcb0854c04a6972eca1f083d8b"
          }
        },
        "c4bdfe0625b64d77b7d906d8087080d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b80821d0b4945058387dd0f6e10fb0c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_66b5069c2b2b46808626b70dbc34f2ab",
            "value": "Batches:‚Äá100%"
          }
        },
        "613fede7d982426ab5b93dc985f444d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e27e5a08746f4e1bb805fad7c75e91df",
            "max": 66,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_268f40dab7b24280b51e8dce9242f752",
            "value": 66
          }
        },
        "9bcb8d7cc9ec4d9fb5f8fc6414cf27c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e31caad6e9140d99ab25298b44ec356",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a5ae9fe165ef4da39e4450df11cea2d1",
            "value": "‚Äá66/66‚Äá[00:00&lt;00:00,‚Äá127.77it/s]"
          }
        },
        "92f196fcb0854c04a6972eca1f083d8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b80821d0b4945058387dd0f6e10fb0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b5069c2b2b46808626b70dbc34f2ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e27e5a08746f4e1bb805fad7c75e91df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "268f40dab7b24280b51e8dce9242f752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e31caad6e9140d99ab25298b44ec356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5ae9fe165ef4da39e4450df11cea2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "# Force immediate flush of print() output\n",
        "from IPython.display import display, clear_output\n"
      ],
      "metadata": {
        "id": "DKzTaKVQmPu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# PRODUCTION-READY COMPLAINT CLASSIFIER\n",
        "# ======================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -----------------------------\n",
        "# 1. DATA PROCESSING\n",
        "# -----------------------------\n",
        "def load_and_clean_data(file_path):\n",
        "    \"\"\"Load and clean the complaint data\"\"\"\n",
        "    df = pd.read_csv(\"/content/processed_complaints_bilingual.csv\")\n",
        "\n",
        "    print(f\"üìä Initial dataset shape: {df.shape}\")\n",
        "    print(\"Category distribution:\")\n",
        "    print(df['Category'].value_counts())\n",
        "\n",
        "    def clean_text(text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text = str(text).strip()\n",
        "        if len(text) < 15:\n",
        "            return \"\"\n",
        "        text = re.sub(r'[^\\w\\s\\u0900-\\u097F!?.,]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    print(\"üßπ Applying proper cleaning...\")\n",
        "    df['cleaned_text'] = df['Standardized_Complaint'].astype(str).apply(clean_text)\n",
        "    df = df[df['cleaned_text'].str.len() > 0]\n",
        "\n",
        "    print(f\"üìä Final dataset: {len(df)} samples\")\n",
        "    print(\"Category distribution:\")\n",
        "    print(df['Category'].value_counts())\n",
        "\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 2. FEATURE ENGINEERING\n",
        "# -----------------------------\n",
        "def create_features(df):\n",
        "    \"\"\"Create features for the model\"\"\"\n",
        "\n",
        "    keyword_groups = {\n",
        "        'water_related': ['water', '‡§™‡§æ‡§£‡•Ä', 'pipe', 'supply', '‡§ü‡§Å‡§ï‡§∞', '‡§®‡§≥', '‡§™‡§æ‡§£‡•Ä‡§™‡•Å‡§∞‡§µ‡§†‡§æ'],\n",
        "        'electricity_related': ['electric', 'power', '‡§µ‡•Ä‡§ú', 'light', 'transformer', '‡§µ‡§ø‡§¶‡•ç‡§Ø‡•Å‡§§', 'outage'],\n",
        "        'road_related': ['road', '‡§∞‡§∏‡•ç‡§§‡§æ', 'pothole', 'repair', '‡§ñ‡§°‡•ç‡§°‡§æ', '‡§¶‡•Å‡§∞‡•Å‡§∏‡•ç‡§§‡•Ä', 'highway'],\n",
        "        'sanitation_related': ['garbage', '‡§ï‡§ö‡§∞‡§æ', 'waste', 'clean', '‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ‡§§‡§æ', 'drain', '‡§°‡•ç‡§∞‡•á‡§®‡•á‡§ú'],\n",
        "        'health_related': ['health', '‡§Ü‡§∞‡•ã‡§ó‡•ç‡§Ø', 'hospital', 'ambulance', 'medical', 'doctor', '‡§¶‡§µ‡§æ‡§ñ‡§æ‡§®‡§æ'],\n",
        "        'education_related': ['school', '‡§∂‡§æ‡§≥‡§æ', 'education', 'teacher', '‡§∂‡§ø‡§ï‡•ç‡§∑‡§£', 'college', '‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•Ä']\n",
        "    }\n",
        "\n",
        "    def create_structural_features(texts):\n",
        "        feature_data = []\n",
        "        for text in texts:\n",
        "            features = []\n",
        "            for domain, words in keyword_groups.items():\n",
        "                present = any(word in text.lower() for word in words)\n",
        "                features.append(1 if present else 0)\n",
        "\n",
        "            features.append(len(text))\n",
        "            features.append(len(text.split()))\n",
        "            features.append(len(text) / max(len(text.split()), 1))\n",
        "\n",
        "            has_marathi = any(char in text for char in '‡§Ö‡§Ü‡§á‡§à‡§â‡§ä‡§ã‡§è‡§ê‡§ì‡§î‡§ï‡§ñ‡§ó‡§ò‡§ô‡§ö‡§õ‡§ú‡§ù‡§û‡§ü‡§†‡§°‡§¢‡§£‡§§‡§•‡§¶‡§ß‡§®‡§™‡§´‡§¨‡§≠‡§Æ‡§Ø‡§∞‡§≤‡§µ‡§∂‡§∑‡§∏‡§π')\n",
        "            features.append(1 if has_marathi else 0)\n",
        "\n",
        "            urgent_words = ['urgent', 'immediate', 'emergency', '‡§§‡§§‡•ç‡§ï‡§æ‡§≥', '‡§§‡•Å‡§∞‡§Ç‡§§', '‡§ú‡§∞‡•Å‡§∞‡•Ä']\n",
        "            has_urgency = any(word in text.lower() for word in urgent_words)\n",
        "            features.append(1 if has_urgency else 0)\n",
        "\n",
        "            has_numbers = bool(re.search(r'\\d+', text))\n",
        "            features.append(1 if has_numbers else 0)\n",
        "\n",
        "            feature_data.append(features)\n",
        "\n",
        "        return np.array(feature_data)\n",
        "\n",
        "    # Create structural features\n",
        "    X_struct = create_structural_features(df['cleaned_text'].tolist())\n",
        "\n",
        "    # Generate embeddings\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"üß† Using device: {device}\")\n",
        "\n",
        "    print(\"üìä Generating embeddings...\")\n",
        "    embed_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "    embeddings = embed_model.encode(\n",
        "        df['cleaned_text'].tolist(),\n",
        "        batch_size=64,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_tensor=False,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "    # Combine features\n",
        "    X_combined = np.hstack([embeddings, X_struct])\n",
        "    print(f\"üöÄ Feature shape: {X_combined.shape}\")\n",
        "\n",
        "    return X_combined, embed_model, keyword_groups\n",
        "\n",
        "# -----------------------------\n",
        "# 3. MODEL TRAINING\n",
        "# -----------------------------\n",
        "def train_model(X_combined, y, test_size=0.3):\n",
        "    \"\"\"Train the classification model\"\"\"\n",
        "\n",
        "    # Split data first to avoid leakage\n",
        "    X_train_full, X_test_full, y_train, y_test = train_test_split(\n",
        "        X_combined, y,\n",
        "        test_size=test_size,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"üìö Training set: {len(X_train_full)} samples\")\n",
        "    print(f\"üß™ Test set: {len(X_test_full)} samples\")\n",
        "\n",
        "    # Feature selection on training data only\n",
        "    selector = SelectKBest(f_classif, k=100)\n",
        "    X_train_selected = selector.fit_transform(X_train_full, y_train)\n",
        "    X_test_selected = selector.transform(X_test_full)\n",
        "\n",
        "    print(f\"üéØ Selected {X_train_selected.shape[1]} features\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    # Train LightGBM model\n",
        "    print(\"\\nüöÄ Training model...\")\n",
        "\n",
        "    lgb_params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': len(np.unique(y)),\n",
        "        'learning_rate': 0.05,\n",
        "        'num_leaves': 31,\n",
        "        'max_depth': 5,\n",
        "        'min_data_in_leaf': 20,\n",
        "        'feature_fraction': 0.7,\n",
        "        'bagging_fraction': 0.7,\n",
        "        'lambda_l1': 1.0,\n",
        "        'lambda_l2': 1.0,\n",
        "        'verbosity': -1,\n",
        "        'device': 'gpu' if torch.cuda.is_available() else 'cpu',\n",
        "    }\n",
        "\n",
        "    train_data = lgb.Dataset(X_train_scaled, label=y_train)\n",
        "    test_data = lgb.Dataset(X_test_scaled, label=y_test, reference=train_data)\n",
        "\n",
        "    lgb_model = lgb.train(\n",
        "        params=lgb_params,\n",
        "        train_set=train_data,\n",
        "        num_boost_round=200,\n",
        "        valid_sets=[test_data],\n",
        "        callbacks=[lgb.early_stopping(20, verbose=True)]\n",
        "    )\n",
        "\n",
        "    return (lgb_model, selector, scaler,\n",
        "            X_train_scaled, X_test_scaled,\n",
        "            y_train, y_test, X_train_selected.shape[1])\n",
        "\n",
        "# -----------------------------\n",
        "# 4. EVALUATION\n",
        "# -----------------------------\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, le):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "\n",
        "    train_preds = np.argmax(model.predict(X_train), axis=1)\n",
        "    test_preds = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, train_preds)\n",
        "    test_accuracy = accuracy_score(y_test, test_preds)\n",
        "    test_f1 = f1_score(y_test, test_preds, average='weighted')\n",
        "\n",
        "    print(\"\\nüìä EVALUATION RESULTS:\")\n",
        "    print(f\"üéØ Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"üéØ Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"üéØ Test Weighted F1: {test_f1:.4f}\")\n",
        "    print(f\"üìä Overfitting gap: {train_accuracy - test_accuracy:.4f}\")\n",
        "\n",
        "    print(f\"\\nüìã Classification Report:\")\n",
        "    print(classification_report(y_test, test_preds, target_names=le.classes_))\n",
        "\n",
        "    return train_accuracy, test_accuracy, test_f1\n",
        "\n",
        "# -----------------------------\n",
        "# 5. PREDICTION PIPELINE\n",
        "# -----------------------------\n",
        "class ComplaintClassifier:\n",
        "    \"\"\"Production-ready complaint classifier\"\"\"\n",
        "\n",
        "    def __init__(self, model, selector, scaler, embed_model, le, keyword_groups, expected_feature_dim):\n",
        "        self.model = model\n",
        "        self.selector = selector\n",
        "        self.scaler = scaler\n",
        "        self.embed_model = embed_model\n",
        "        self.le = le\n",
        "        self.keyword_groups = keyword_groups\n",
        "        self.expected_feature_dim = expected_feature_dim\n",
        "\n",
        "        # Education boosting patterns\n",
        "        self.education_patterns = {\n",
        "            'school_infrastructure': [\n",
        "                (r'school\\s+(building|roof|wall|classroom)', 3),\n",
        "                (r'‡§∂‡§æ‡§≥.*(‡§á‡§Æ‡§æ‡§∞‡§§|‡§õ‡§§|‡§≠‡§ø‡§Ç‡§§|‡§µ‡§∞‡•ç‡§ó)', 3),\n",
        "                (r'building.*repair', 2),\n",
        "                (r'‡§á‡§Æ‡§æ‡§∞‡§§.*‡§¶‡•Å‡§∞‡•Å‡§∏‡•ç‡§§‡•Ä', 2)\n",
        "            ],\n",
        "            'teachers_staff': [\n",
        "                (r'teacher.*not.*available', 3),\n",
        "                (r'‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï.*‡§®‡§æ‡§π‡•Ä', 3),\n",
        "                (r'no.*teacher', 2),\n",
        "                (r'staff.*absent', 2)\n",
        "            ],\n",
        "            'student_amenities': [\n",
        "                (r'midday\\s+meal', 3),\n",
        "                (r'‡§Æ‡§ß‡•ç‡§Ø‡§æ‡§®‡•ç‡§π\\s+‡§≠‡•ã‡§ú‡§®', 3),\n",
        "                (r'books.*not', 2),\n",
        "                (r'‡§™‡•Å‡§∏‡•ç‡§§‡§ï.*‡§®‡§æ‡§π‡•Ä', 2),\n",
        "                (r'uniform', 1),\n",
        "                (r'‡§ó‡§£‡§µ‡•á‡§∂', 1)\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean input text\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text = str(text).strip()\n",
        "        if len(text) < 15:\n",
        "            return \"\"\n",
        "        text = re.sub(r'[^\\w\\s\\u0900-\\u097F!?.,]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def create_features_for_prediction(self, text):\n",
        "        \"\"\"Create features for a single text\"\"\"\n",
        "        cleaned_text = self.clean_text(text)\n",
        "\n",
        "        # Structural features\n",
        "        features = []\n",
        "        for domain, words in self.keyword_groups.items():\n",
        "            present = any(word in cleaned_text.lower() for word in words)\n",
        "            features.append(1 if present else 0)\n",
        "\n",
        "        features.append(len(cleaned_text))\n",
        "        features.append(len(cleaned_text.split()))\n",
        "        features.append(len(cleaned_text) / max(len(cleaned_text.split()), 1))\n",
        "\n",
        "        has_marathi = any(char in cleaned_text for char in '‡§Ö‡§Ü‡§á‡§à‡§â‡§ä‡§ã‡§è‡§ê‡§ì‡§î‡§ï‡§ñ‡§ó‡§ò‡§ô‡§ö‡§õ‡§ú‡§ù‡§û‡§ü‡§†‡§°‡§¢‡§£‡§§‡§•‡§¶‡§ß‡§®‡§™‡§´‡§¨‡§≠‡§Æ‡§Ø‡§∞‡§≤‡§µ‡§∂‡§∑‡§∏‡§π')\n",
        "        features.append(1 if has_marathi else 0)\n",
        "\n",
        "        urgent_words = ['urgent', 'immediate', 'emergency', '‡§§‡§§‡•ç‡§ï‡§æ‡§≥', '‡§§‡•Å‡§∞‡§Ç‡§§', '‡§ú‡§∞‡•Å‡§∞‡•Ä']\n",
        "        has_urgency = any(word in cleaned_text.lower() for word in urgent_words)\n",
        "        features.append(1 if has_urgency else 0)\n",
        "\n",
        "        has_numbers = bool(re.search(r'\\d+', cleaned_text))\n",
        "        features.append(1 if has_numbers else 0)\n",
        "\n",
        "        structural_features = np.array([features])\n",
        "\n",
        "        # Generate embedding\n",
        "        text_embedding = self.embed_model.encode([cleaned_text])\n",
        "\n",
        "        # Combine features\n",
        "        feature_combined = np.hstack([text_embedding, structural_features])\n",
        "\n",
        "        return feature_combined, cleaned_text\n",
        "\n",
        "    def predict(self, text, confidence_threshold=0.6, use_education_boost=True):\n",
        "        \"\"\"Predict category for a complaint text\"\"\"\n",
        "        try:\n",
        "            if len(self.clean_text(text)) < 15:\n",
        "                return \"Unknown\", 0.0\n",
        "\n",
        "            # Create features\n",
        "            feature_combined, cleaned_text = self.create_features_for_prediction(text)\n",
        "\n",
        "            # Apply feature selection and scaling\n",
        "            feature_selected = self.selector.transform(feature_combined)\n",
        "            feature_scaled = self.scaler.transform(feature_selected)\n",
        "\n",
        "            # Predict\n",
        "            probs = self.model.predict(feature_scaled)\n",
        "            pred_idx = np.argmax(probs)\n",
        "            confidence = probs[0][pred_idx]\n",
        "            predicted_class = self.le.classes_[pred_idx]\n",
        "\n",
        "            # Apply education boosting if enabled\n",
        "            if use_education_boost:\n",
        "                predicted_class, confidence = self._apply_education_boost(\n",
        "                    cleaned_text, predicted_class, confidence, probs[0]\n",
        "                )\n",
        "\n",
        "            # Apply confidence threshold\n",
        "            if confidence < confidence_threshold:\n",
        "                return \"Uncertain\", confidence\n",
        "\n",
        "            return predicted_class, float(confidence)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Prediction error: {e}\")\n",
        "            return \"Unknown\", 0.0\n",
        "\n",
        "    def _apply_education_boost(self, text, current_class, current_confidence, all_probs,\n",
        "                             education_boost=2.0, min_education_prob=0.2):\n",
        "        \"\"\"Apply education class boosting\"\"\"\n",
        "        if current_class == \"Education\" or current_class in [\"Uncertain\", \"Unknown\"]:\n",
        "            return current_class, current_confidence\n",
        "\n",
        "        # Check education signals\n",
        "        education_score = 0\n",
        "        cleaned_text = text.lower()\n",
        "\n",
        "        for pattern_type, patterns in self.education_patterns.items():\n",
        "            for pattern, weight in patterns:\n",
        "                if re.search(pattern, cleaned_text, re.IGNORECASE):\n",
        "                    education_score += weight\n",
        "\n",
        "        # Apply boost if strong education signals\n",
        "        if education_score >= 2:\n",
        "            education_idx = list(self.le.classes_).index('Education')\n",
        "            education_prob = all_probs[education_idx]\n",
        "\n",
        "            if education_prob >= min_education_prob:\n",
        "                boosted_confidence = min(education_prob * education_boost, 0.95)\n",
        "                if boosted_confidence > current_confidence:\n",
        "                    return \"Education\", boosted_confidence\n",
        "\n",
        "        return current_class, current_confidence\n",
        "\n",
        "    def predict_batch(self, texts, confidence_threshold=0.6, use_education_boost=True):\n",
        "        \"\"\"Predict categories for multiple texts\"\"\"\n",
        "        results = []\n",
        "        for text in texts:\n",
        "            category, confidence = self.predict(text, confidence_threshold, use_education_boost)\n",
        "            results.append((category, confidence))\n",
        "        return results\n",
        "\n",
        "# -----------------------------\n",
        "# 6. MAIN EXECUTION\n",
        "# -----------------------------\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Load and prepare data\n",
        "    DATA_PATH = \"/content/processed_complaints_bilingual.csv\"\n",
        "    df = load_and_clean_data(DATA_PATH)\n",
        "\n",
        "    # Encode target\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(df['Category'])\n",
        "    num_classes = len(le.classes_)\n",
        "    print(f\"üéØ Number of classes: {num_classes}\")\n",
        "\n",
        "    # Create features\n",
        "    X_combined, embed_model, keyword_groups = create_features(df)\n",
        "\n",
        "    # Train model\n",
        "    (lgb_model, selector, scaler,\n",
        "     X_train_scaled, X_test_scaled,\n",
        "     y_train, y_test, feature_dim) = train_model(X_combined, y)\n",
        "\n",
        "    # Evaluate model\n",
        "    train_acc, test_acc, test_f1 = evaluate_model(\n",
        "        lgb_model, X_train_scaled, X_test_scaled, y_train, y_test, le\n",
        "    )\n",
        "\n",
        "    # Create classifier instance\n",
        "    classifier = ComplaintClassifier(\n",
        "        model=lgb_model,\n",
        "        selector=selector,\n",
        "        scaler=scaler,\n",
        "        embed_model=embed_model,\n",
        "        le=le,\n",
        "        keyword_groups=keyword_groups,\n",
        "        expected_feature_dim=feature_dim\n",
        "    )\n",
        "\n",
        "    # Test predictions\n",
        "    print(\"\\nüß™ Testing Predictions...\")\n",
        "\n",
        "    test_cases = [\n",
        "        'No water supply in our village for 3 days, urgent help needed immediately!',\n",
        "        '‡§ó‡§æ‡§µ‡§æ‡§§ ‡§µ‡•Ä‡§ú ‡§™‡•Å‡§∞‡§µ‡§†‡§æ ‡§ñ‡§Ç‡§°‡§ø‡§§ ‡§ù‡§æ‡§≤‡§æ ‡§Ü‡§π‡•á, ‡§§‡§æ‡§§‡§°‡•Ä‡§®‡•á ‡§¶‡•Å‡§∞‡•Å‡§∏‡•ç‡§§‡•Ä ‡§π‡§µ‡•Ä',\n",
        "        'Road full of potholes needs immediate repair before monsoon season',\n",
        "        '‡§™‡§æ‡§£‡•Ä ‡§ü‡§Å‡§ï‡§∞‡§ö‡•Ä ‡§§‡§æ‡§§‡§°‡•Ä‡§ö‡•Ä ‡§Æ‡§æ‡§ó‡§£‡•Ä, ‡§ó‡§æ‡§µ‡§æ‡§§ ‡§ó‡§Ç‡§≠‡•Ä‡§∞ ‡§™‡§æ‡§£‡•Ä‡§ü‡§Ç‡§ö‡§æ‡§à ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§ù‡§æ‡§≤‡•Ä ‡§Ü‡§π‡•á',\n",
        "        'Garbage not collected for 15 days causing serious health hazard',\n",
        "        'Street lights not working in our area for 3 weeks',\n",
        "        'School building roof damaged, needs urgent repair',\n",
        "        'Transformer blast caused electricity outage in entire village'\n",
        "    ]\n",
        "\n",
        "    print(\"\\nüìù Prediction Results:\")\n",
        "    for i, text in enumerate(test_cases, 1):\n",
        "        category, confidence = classifier.predict(text)\n",
        "        confidence_status = \"HIGH\" if confidence > 0.7 else \"MEDIUM\" if confidence > 0.5 else \"LOW\"\n",
        "        print(f\"{i:2d}. {category:<12} ({confidence:.3f} - {confidence_status}): {text[:50]}...\")\n",
        "\n",
        "    # Save pipeline - CORRECTED: Save everything in one file\n",
        "    print(\"\\nüíæ Saving Pipeline...\")\n",
        "\n",
        "    # Save complete pipeline in one file\n",
        "    pipeline_data = {\n",
        "        'model': lgb_model,\n",
        "        'selector': selector,\n",
        "        'scaler': scaler,\n",
        "        'embed_model': embed_model,\n",
        "        'le': le,\n",
        "        'keyword_groups': keyword_groups,\n",
        "        'expected_feature_dim': feature_dim,\n",
        "        'performance': {\n",
        "            'test_accuracy': test_acc,\n",
        "            'test_f1': test_f1,\n",
        "            'num_classes': num_classes,\n",
        "            'dataset_size': len(df)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    joblib.dump(pipeline_data, \"complaint_classifier_pipeline.pkl\")\n",
        "    print(\"‚úÖ Pipeline saved successfully!\")\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\nüéâ PRODUCTION CLASSIFIER READY!\")\n",
        "    print(f\"üìä Dataset: {len(df)} samples\")\n",
        "    print(f\"üéØ Test Accuracy: {test_acc:.1%}\")\n",
        "    print(f\"üìà Weighted F1: {test_f1:.1%}\")\n",
        "    print(f\"üîß Features: {feature_dim} dimensions\")\n",
        "    print(f\"üöÄ Education boosting implemented!\")\n",
        "\n",
        "    return classifier\n",
        "\n",
        "# -----------------------------\n",
        "# 7. LOADING FUNCTION - CORRECTED\n",
        "# -----------------------------\n",
        "def load_classifier(pipeline_path=\"complaint_classifier_pipeline.pkl\"):\n",
        "    \"\"\"Load saved classifier pipeline - CORRECTED VERSION\"\"\"\n",
        "    try:\n",
        "        # Load everything from one file\n",
        "        pipeline_data = joblib.load(pipeline_path)\n",
        "\n",
        "        # Create classifier instance with loaded components\n",
        "        classifier = ComplaintClassifier(\n",
        "            model=pipeline_data['model'],\n",
        "            selector=pipeline_data['selector'],\n",
        "            scaler=pipeline_data['scaler'],\n",
        "            embed_model=pipeline_data['embed_model'],\n",
        "            le=pipeline_data['le'],\n",
        "            keyword_groups=pipeline_data['keyword_groups'],\n",
        "            expected_feature_dim=pipeline_data['expected_feature_dim']\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Classifier loaded successfully!\")\n",
        "        print(f\"üìä Model Performance: {pipeline_data['performance']['test_accuracy']:.1%} accuracy\")\n",
        "\n",
        "        return classifier\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading classifier: {e}\")\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# 8. USAGE EXAMPLE\n",
        "# -----------------------------\n",
        "def usage_example():\n",
        "    \"\"\"Show how to use the classifier\"\"\"\n",
        "    print(\"\\nüîß USAGE EXAMPLE:\")\n",
        "\n",
        "    # Load the classifier\n",
        "    classifier = load_classifier()\n",
        "\n",
        "    if classifier is not None:\n",
        "        # Single prediction\n",
        "        complaint = \"Water pipe broken in sector 5, need immediate repair\"\n",
        "        category, confidence = classifier.predict(complaint)\n",
        "        print(f\"Single prediction: '{complaint}' -> {category} (confidence: {confidence:.3f})\")\n",
        "\n",
        "        # Batch prediction\n",
        "        complaints = [\n",
        "            \"No electricity in our area for 2 days\",\n",
        "            \"School building needs urgent repair\",\n",
        "            \"Garbage not collected for 1 week\"\n",
        "        ]\n",
        "        results = classifier.predict_batch(complaints)\n",
        "        print(\"\\nBatch predictions:\")\n",
        "        for i, (complaint, (category, confidence)) in enumerate(zip(complaints, results), 1):\n",
        "            print(f\"{i}. '{complaint[:30]}...' -> {category} ({confidence:.3f})\")\n",
        "\n",
        "# -----------------------------\n",
        "# RUN THE PIPELINE\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Train and save the model\n",
        "    classifier = main()\n",
        "\n",
        "    # Show usage example\n",
        "    usage_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "84152514235842b5b7d79caece4c1ba6",
            "c4bdfe0625b64d77b7d906d8087080d8",
            "613fede7d982426ab5b93dc985f444d3",
            "9bcb8d7cc9ec4d9fb5f8fc6414cf27c6",
            "92f196fcb0854c04a6972eca1f083d8b",
            "9b80821d0b4945058387dd0f6e10fb0c",
            "66b5069c2b2b46808626b70dbc34f2ab",
            "e27e5a08746f4e1bb805fad7c75e91df",
            "268f40dab7b24280b51e8dce9242f752",
            "9e31caad6e9140d99ab25298b44ec356",
            "a5ae9fe165ef4da39e4450df11cea2d1"
          ]
        },
        "id": "Fh6B2bU4VhaB",
        "outputId": "056ce56e-6e15-496d-ade9-386d1998de2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Initial dataset shape: (7307, 9)\n",
            "Category distribution:\n",
            "Category\n",
            "Health         1260\n",
            "Sanitation     1230\n",
            "Road           1216\n",
            "Water          1212\n",
            "Electricity    1196\n",
            "Others          688\n",
            "Education       505\n",
            "Name: count, dtype: int64\n",
            "üßπ Applying proper cleaning...\n",
            "üìä Final dataset: 4196 samples\n",
            "Category distribution:\n",
            "Category\n",
            "Health         833\n",
            "Water          819\n",
            "Sanitation     810\n",
            "Electricity    780\n",
            "Others         423\n",
            "Road           390\n",
            "Education      141\n",
            "Name: count, dtype: int64\n",
            "üéØ Number of classes: 7\n",
            "üß† Using device: cuda\n",
            "üìä Generating embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/66 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84152514235842b5b7d79caece4c1ba6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Feature shape: (4196, 396)\n",
            "üìö Training set: 2937 samples\n",
            "üß™ Test set: 1259 samples\n",
            "üéØ Selected 100 features\n",
            "\n",
            "üöÄ Training model...\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[200]\tvalid_0's multi_logloss: 0.470012\n",
            "\n",
            "üìä EVALUATION RESULTS:\n",
            "üéØ Training Accuracy: 0.8533\n",
            "üéØ Test Accuracy: 0.8261\n",
            "üéØ Test Weighted F1: 0.8175\n",
            "üìä Overfitting gap: 0.0272\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Education       0.00      0.00      0.00        42\n",
            " Electricity       1.00      0.79      0.88       234\n",
            "      Health       0.68      0.93      0.79       250\n",
            "      Others       1.00      1.00      1.00       127\n",
            "        Road       1.00      0.66      0.79       117\n",
            "  Sanitation       1.00      0.81      0.89       243\n",
            "       Water       0.67      0.91      0.77       246\n",
            "\n",
            "    accuracy                           0.83      1259\n",
            "   macro avg       0.76      0.73      0.73      1259\n",
            "weighted avg       0.84      0.83      0.82      1259\n",
            "\n",
            "\n",
            "üß™ Testing Predictions...\n",
            "\n",
            "üìù Prediction Results:\n",
            " 1. Water        (0.712 - HIGH): No water supply in our village for 3 days, urgent ...\n",
            " 2. Uncertain    (0.395 - LOW): ‡§ó‡§æ‡§µ‡§æ‡§§ ‡§µ‡•Ä‡§ú ‡§™‡•Å‡§∞‡§µ‡§†‡§æ ‡§ñ‡§Ç‡§°‡§ø‡§§ ‡§ù‡§æ‡§≤‡§æ ‡§Ü‡§π‡•á, ‡§§‡§æ‡§§‡§°‡•Ä‡§®‡•á ‡§¶‡•Å‡§∞‡•Å‡§∏‡•ç‡§§‡•Ä ...\n",
            " 3. Uncertain    (0.294 - LOW): Road full of potholes needs immediate repair befor...\n",
            " 4. Uncertain    (0.322 - LOW): ‡§™‡§æ‡§£‡•Ä ‡§ü‡§Å‡§ï‡§∞‡§ö‡•Ä ‡§§‡§æ‡§§‡§°‡•Ä‡§ö‡•Ä ‡§Æ‡§æ‡§ó‡§£‡•Ä, ‡§ó‡§æ‡§µ‡§æ‡§§ ‡§ó‡§Ç‡§≠‡•Ä‡§∞ ‡§™‡§æ‡§£‡•Ä‡§ü‡§Ç‡§ö‡§æ‡§à ‡§®...\n",
            " 5. Uncertain    (0.436 - LOW): Garbage not collected for 15 days causing serious ...\n",
            " 6. Others       (0.671 - MEDIUM): Street lights not working in our area for 3 weeks...\n",
            " 7. Uncertain    (0.492 - LOW): School building roof damaged, needs urgent repair...\n",
            " 8. Electricity  (0.994 - HIGH): Transformer blast caused electricity outage in ent...\n",
            "\n",
            "üíæ Saving Pipeline...\n",
            "‚úÖ Pipeline saved successfully!\n",
            "\n",
            "üéâ PRODUCTION CLASSIFIER READY!\n",
            "üìä Dataset: 4196 samples\n",
            "üéØ Test Accuracy: 82.6%\n",
            "üìà Weighted F1: 81.8%\n",
            "üîß Features: 100 dimensions\n",
            "üöÄ Education boosting implemented!\n",
            "\n",
            "üîß USAGE EXAMPLE:\n",
            "‚úÖ Classifier loaded successfully!\n",
            "üìä Model Performance: 82.6% accuracy\n",
            "Single prediction: 'Water pipe broken in sector 5, need immediate repair' -> Water (confidence: 0.911)\n",
            "\n",
            "Batch predictions:\n",
            "1. 'No electricity in our area for...' -> Uncertain (0.512)\n",
            "2. 'School building needs urgent r...' -> Others (0.941)\n",
            "3. 'Garbage not collected for 1 we...' -> Sanitation (0.931)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wegIpRC6mJm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95KqnDkznqmQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}